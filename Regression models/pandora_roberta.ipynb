{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\youss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism'],\n",
       "        num_rows: 16047\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism'],\n",
       "        num_rows: 2415\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism'],\n",
       "        num_rows: 2415\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Fatima0923/Automated-Personality-Prediction\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['his name was kim kimble originally wow thats some messed up parents',\n",
       "  'theyre better than the normal posts on ryugioh id rather have them then the same topic posted multiple times in the week after the banlist'],\n",
       " 'agreeableness': [9.0, 50.0],\n",
       " 'openness': [61.0, 85.0],\n",
       " 'conscientiousness': [13.0, 50.0],\n",
       " 'extraversion': [4.0, 85.0],\n",
       " 'neuroticism': [72.0, 50.0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his name was kim kimble originally wow thats some messed up parents',\n",
       " 'theyre better than the normal posts on ryugioh id rather have them then the same topic posted multiple times in the week after the banlist',\n",
       " 'how the fuck does this even happen hi youre cute you too ive had a crush on you for awhile um i uh inserts finger in butthole']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = dataset['train']['text']\n",
    "val_text = dataset['validation']['text']\n",
    "test_text = dataset['test']['text']\n",
    "train_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = dataset['train']['agreeableness']\n",
    "label2 = dataset['train']['openness']\n",
    "label3 = dataset['train']['conscientiousness']\n",
    "label4 = dataset['train']['extraversion']\n",
    "label5 = dataset['train']['neuroticism']\n",
    "train_labels = [[a, b, c, d, e] for a, b, c, d, e in zip(label1, label2, label3, label4, label5)]\n",
    "# the labels range from 0-100 so we need to normalize them to 0-1\n",
    "train_labels = torch.tensor(train_labels)/100\n",
    "\n",
    "\n",
    "label1 =dataset['validation']['agreeableness']\n",
    "label2 = dataset['validation']['openness']\n",
    "label3 = dataset['validation']['conscientiousness']\n",
    "label4 = dataset['validation']['extraversion']\n",
    "label5 = dataset['validation']['neuroticism']\n",
    "val_labels = [[a, b, c, d, e] for a, b, c, d, e in zip(label1, label2, label3, label4, label5)]\n",
    "val_labels = torch.tensor(val_labels)/100\n",
    "\n",
    "label1 = dataset['test']['agreeableness']\n",
    "label2 = dataset['test']['openness']\n",
    "label3 = dataset['test']['conscientiousness']\n",
    "label4 = dataset['test']['extraversion']\n",
    "label5 = dataset['test']['neuroticism']\n",
    "test_labels = [[a, b, c, d, e] for a, b, c, d, e in zip(label1, label2, label3, label4, label5)]\n",
    "test_labels = torch.tensor(test_labels)/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for data in soup(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "# Load spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + ['hi', 'im']\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [clean_string(text, \"Spacy\") for text in train_text]\n",
    "val_text = [clean_string(text, \"Spacy\") for text in val_text]\n",
    "test_text = [clean_string(text, \"Spacy\") for text in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Bert Model transformer with a sequence regression head on top (a linear\n",
    "# layer on top of the pooled output).\n",
    "class RobertaForSequenceRegression(nn.Module):\n",
    "  def __init__(self, bert_pretrained_model_name, output_size):\n",
    "    super(RobertaForSequenceRegression, self).__init__()\n",
    "    self.roberta = RobertaModel.from_pretrained(bert_pretrained_model_name,\n",
    "                                          output_attentions = False,\n",
    "                                          output_hidden_states = False)\n",
    "    self.out = nn.Linear(self.roberta.config.hidden_size, output_size)\n",
    "    \n",
    "  def forward(self, input_sentence, input_mask):\n",
    "    last_hidden_states = self.roberta(input_sentence, attention_mask=input_mask)[1]\n",
    "    # pooler_output = self.roberta(input_sentence, attention_mask=input_mask).pooler_output\n",
    "    output = self.out(last_hidden_states)\n",
    "    return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = \"FacebookAI/roberta-base\"\n",
    "model = RobertaForSequenceRegression(checkpoint, output_size=5)\n",
    "\n",
    "# freeze some layers (top | middle | bottom):\n",
    "# bottom = range(2, 12)\n",
    "# middle = list(range(0,5))+list(range(7,12))\n",
    "# top = range(0, 10)\n",
    "\n",
    "# layersToFreeze = top\n",
    "# for i in layersToFreeze:\n",
    "#   print(i)\n",
    "#   for param in model.bert.encoder.layer[i].parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Tell pytorch to run this model on the GPU\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        if len(text) >= 512:\n",
    "            encoding = self.tokenizer(text, return_tensors='pt', truncation=True, max_length = 512, padding='max_length')\n",
    "        else:\n",
    "            encoding = self.tokenizer(text, return_tensors='pt', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = TextClassificationDataset(train_text, train_labels, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "val_dataset = TextClassificationDataset(val_text, val_labels, tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "test_dataset = TextClassificationDataset(test_text, test_labels, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_13976\\86404554.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0191,  0.0324, -0.0294,  0.1250, -0.1512],\n",
       "        [ 0.0221,  0.0296, -0.0263,  0.1346, -0.1641],\n",
       "        [ 0.0221,  0.0442, -0.0431,  0.1259, -0.1536],\n",
       "        [ 0.0202,  0.0446, -0.0392,  0.1202, -0.1606]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "b_input_ids = batch['input_ids']\n",
    "b_input_mask = batch['attention_mask']\n",
    "b_labels = batch['labels']\n",
    "\n",
    "model(b_input_ids, b_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceRegression(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_13976\\86404554.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  4,012.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  4,012.    Elapsed: 0:00:14.\n",
      "  Batch   120  of  4,012.    Elapsed: 0:00:21.\n",
      "  Batch   160  of  4,012.    Elapsed: 0:00:29.\n",
      "  Batch   200  of  4,012.    Elapsed: 0:00:36.\n",
      "  Batch   240  of  4,012.    Elapsed: 0:00:43.\n",
      "  Batch   280  of  4,012.    Elapsed: 0:00:49.\n",
      "  Batch   320  of  4,012.    Elapsed: 0:00:57.\n",
      "  Batch   360  of  4,012.    Elapsed: 0:01:03.\n",
      "  Batch   400  of  4,012.    Elapsed: 0:01:10.\n",
      "  Batch   440  of  4,012.    Elapsed: 0:01:16.\n",
      "  Batch   480  of  4,012.    Elapsed: 0:01:24.\n",
      "  Batch   520  of  4,012.    Elapsed: 0:01:32.\n",
      "  Batch   560  of  4,012.    Elapsed: 0:01:41.\n",
      "  Batch   600  of  4,012.    Elapsed: 0:01:47.\n",
      "  Batch   640  of  4,012.    Elapsed: 0:01:52.\n",
      "  Batch   680  of  4,012.    Elapsed: 0:01:59.\n",
      "  Batch   720  of  4,012.    Elapsed: 0:02:07.\n",
      "  Batch   760  of  4,012.    Elapsed: 0:02:13.\n",
      "  Batch   800  of  4,012.    Elapsed: 0:02:20.\n",
      "  Batch   840  of  4,012.    Elapsed: 0:02:26.\n",
      "  Batch   880  of  4,012.    Elapsed: 0:02:33.\n",
      "  Batch   920  of  4,012.    Elapsed: 0:02:40.\n",
      "  Batch   960  of  4,012.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  4,012.    Elapsed: 0:02:52.\n",
      "  Batch 1,040  of  4,012.    Elapsed: 0:03:00.\n",
      "  Batch 1,080  of  4,012.    Elapsed: 0:03:07.\n",
      "  Batch 1,120  of  4,012.    Elapsed: 0:03:14.\n",
      "  Batch 1,160  of  4,012.    Elapsed: 0:03:20.\n",
      "  Batch 1,200  of  4,012.    Elapsed: 0:03:29.\n",
      "  Batch 1,240  of  4,012.    Elapsed: 0:03:35.\n",
      "  Batch 1,280  of  4,012.    Elapsed: 0:03:41.\n",
      "  Batch 1,320  of  4,012.    Elapsed: 0:03:49.\n",
      "  Batch 1,360  of  4,012.    Elapsed: 0:03:55.\n",
      "  Batch 1,400  of  4,012.    Elapsed: 0:04:02.\n",
      "  Batch 1,440  of  4,012.    Elapsed: 0:04:09.\n",
      "  Batch 1,480  of  4,012.    Elapsed: 0:04:15.\n",
      "  Batch 1,520  of  4,012.    Elapsed: 0:04:20.\n",
      "  Batch 1,560  of  4,012.    Elapsed: 0:04:27.\n",
      "  Batch 1,600  of  4,012.    Elapsed: 0:04:35.\n",
      "  Batch 1,640  of  4,012.    Elapsed: 0:04:42.\n",
      "  Batch 1,680  of  4,012.    Elapsed: 0:04:49.\n",
      "  Batch 1,720  of  4,012.    Elapsed: 0:04:56.\n",
      "  Batch 1,760  of  4,012.    Elapsed: 0:05:04.\n",
      "  Batch 1,800  of  4,012.    Elapsed: 0:05:12.\n",
      "  Batch 1,840  of  4,012.    Elapsed: 0:05:18.\n",
      "  Batch 1,880  of  4,012.    Elapsed: 0:05:24.\n",
      "  Batch 1,920  of  4,012.    Elapsed: 0:05:29.\n",
      "  Batch 1,960  of  4,012.    Elapsed: 0:05:35.\n",
      "  Batch 2,000  of  4,012.    Elapsed: 0:05:43.\n",
      "  Batch 2,040  of  4,012.    Elapsed: 0:05:49.\n",
      "  Batch 2,080  of  4,012.    Elapsed: 0:05:57.\n",
      "  Batch 2,120  of  4,012.    Elapsed: 0:06:04.\n",
      "  Batch 2,160  of  4,012.    Elapsed: 0:06:12.\n",
      "  Batch 2,200  of  4,012.    Elapsed: 0:06:20.\n",
      "  Batch 2,240  of  4,012.    Elapsed: 0:06:27.\n",
      "  Batch 2,280  of  4,012.    Elapsed: 0:06:35.\n",
      "  Batch 2,320  of  4,012.    Elapsed: 0:06:41.\n",
      "  Batch 2,360  of  4,012.    Elapsed: 0:06:47.\n",
      "  Batch 2,400  of  4,012.    Elapsed: 0:06:55.\n",
      "  Batch 2,440  of  4,012.    Elapsed: 0:07:02.\n",
      "  Batch 2,480  of  4,012.    Elapsed: 0:07:08.\n",
      "  Batch 2,520  of  4,012.    Elapsed: 0:07:14.\n",
      "  Batch 2,560  of  4,012.    Elapsed: 0:07:21.\n",
      "  Batch 2,600  of  4,012.    Elapsed: 0:07:28.\n",
      "  Batch 2,640  of  4,012.    Elapsed: 0:07:37.\n",
      "  Batch 2,680  of  4,012.    Elapsed: 0:07:45.\n",
      "  Batch 2,720  of  4,012.    Elapsed: 0:07:53.\n",
      "  Batch 2,760  of  4,012.    Elapsed: 0:08:01.\n",
      "  Batch 2,800  of  4,012.    Elapsed: 0:08:08.\n",
      "  Batch 2,840  of  4,012.    Elapsed: 0:08:15.\n",
      "  Batch 2,880  of  4,012.    Elapsed: 0:08:22.\n",
      "  Batch 2,920  of  4,012.    Elapsed: 0:08:28.\n",
      "  Batch 2,960  of  4,012.    Elapsed: 0:08:34.\n",
      "  Batch 3,000  of  4,012.    Elapsed: 0:08:40.\n",
      "  Batch 3,040  of  4,012.    Elapsed: 0:08:45.\n",
      "  Batch 3,080  of  4,012.    Elapsed: 0:08:51.\n",
      "  Batch 3,120  of  4,012.    Elapsed: 0:08:58.\n",
      "  Batch 3,160  of  4,012.    Elapsed: 0:09:05.\n",
      "  Batch 3,200  of  4,012.    Elapsed: 0:09:14.\n",
      "  Batch 3,240  of  4,012.    Elapsed: 0:09:21.\n",
      "  Batch 3,280  of  4,012.    Elapsed: 0:09:27.\n",
      "  Batch 3,320  of  4,012.    Elapsed: 0:09:36.\n",
      "  Batch 3,360  of  4,012.    Elapsed: 0:09:42.\n",
      "  Batch 3,400  of  4,012.    Elapsed: 0:09:52.\n",
      "  Batch 3,440  of  4,012.    Elapsed: 0:09:59.\n",
      "  Batch 3,480  of  4,012.    Elapsed: 0:10:05.\n",
      "  Batch 3,520  of  4,012.    Elapsed: 0:10:13.\n",
      "  Batch 3,560  of  4,012.    Elapsed: 0:10:19.\n",
      "  Batch 3,600  of  4,012.    Elapsed: 0:10:27.\n",
      "  Batch 3,640  of  4,012.    Elapsed: 0:10:35.\n",
      "  Batch 3,680  of  4,012.    Elapsed: 0:10:42.\n",
      "  Batch 3,720  of  4,012.    Elapsed: 0:10:48.\n",
      "  Batch 3,760  of  4,012.    Elapsed: 0:10:55.\n",
      "  Batch 3,800  of  4,012.    Elapsed: 0:11:02.\n",
      "  Batch 3,840  of  4,012.    Elapsed: 0:11:11.\n",
      "  Batch 3,880  of  4,012.    Elapsed: 0:11:18.\n",
      "  Batch 3,920  of  4,012.    Elapsed: 0:11:26.\n",
      "  Batch 3,960  of  4,012.    Elapsed: 0:11:33.\n",
      "  Batch 4,000  of  4,012.    Elapsed: 0:11:39.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:11:42\n",
      "\n",
      "Running Validation...\n",
      "Saving model...\n",
      "  Validation Loss: 0.08\n",
      "  Validation took: 0:00:40\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  4,012.    Elapsed: 0:00:07.\n",
      "  Batch    80  of  4,012.    Elapsed: 0:00:13.\n",
      "  Batch   120  of  4,012.    Elapsed: 0:00:22.\n",
      "  Batch   160  of  4,012.    Elapsed: 0:00:28.\n",
      "  Batch   200  of  4,012.    Elapsed: 0:00:35.\n",
      "  Batch   240  of  4,012.    Elapsed: 0:00:42.\n",
      "  Batch   280  of  4,012.    Elapsed: 0:00:48.\n",
      "  Batch   320  of  4,012.    Elapsed: 0:00:53.\n",
      "  Batch   360  of  4,012.    Elapsed: 0:01:01.\n",
      "  Batch   400  of  4,012.    Elapsed: 0:01:07.\n",
      "  Batch   440  of  4,012.    Elapsed: 0:01:14.\n",
      "  Batch   480  of  4,012.    Elapsed: 0:01:20.\n",
      "  Batch   520  of  4,012.    Elapsed: 0:01:26.\n",
      "  Batch   560  of  4,012.    Elapsed: 0:01:33.\n",
      "  Batch   600  of  4,012.    Elapsed: 0:01:41.\n",
      "  Batch   640  of  4,012.    Elapsed: 0:01:49.\n",
      "  Batch   680  of  4,012.    Elapsed: 0:01:55.\n",
      "  Batch   720  of  4,012.    Elapsed: 0:02:03.\n",
      "  Batch   760  of  4,012.    Elapsed: 0:02:08.\n",
      "  Batch   800  of  4,012.    Elapsed: 0:02:17.\n",
      "  Batch   840  of  4,012.    Elapsed: 0:02:23.\n",
      "  Batch   880  of  4,012.    Elapsed: 0:02:31.\n",
      "  Batch   920  of  4,012.    Elapsed: 0:02:38.\n",
      "  Batch   960  of  4,012.    Elapsed: 0:02:44.\n",
      "  Batch 1,000  of  4,012.    Elapsed: 0:02:51.\n",
      "  Batch 1,040  of  4,012.    Elapsed: 0:03:00.\n",
      "  Batch 1,080  of  4,012.    Elapsed: 0:03:06.\n",
      "  Batch 1,120  of  4,012.    Elapsed: 0:03:13.\n",
      "  Batch 1,160  of  4,012.    Elapsed: 0:03:20.\n",
      "  Batch 1,200  of  4,012.    Elapsed: 0:03:28.\n",
      "  Batch 1,240  of  4,012.    Elapsed: 0:03:35.\n",
      "  Batch 1,280  of  4,012.    Elapsed: 0:03:42.\n",
      "  Batch 1,320  of  4,012.    Elapsed: 0:03:48.\n",
      "  Batch 1,360  of  4,012.    Elapsed: 0:03:54.\n",
      "  Batch 1,400  of  4,012.    Elapsed: 0:04:01.\n",
      "  Batch 1,440  of  4,012.    Elapsed: 0:04:08.\n",
      "  Batch 1,480  of  4,012.    Elapsed: 0:04:14.\n",
      "  Batch 1,520  of  4,012.    Elapsed: 0:04:21.\n",
      "  Batch 1,560  of  4,012.    Elapsed: 0:04:28.\n",
      "  Batch 1,600  of  4,012.    Elapsed: 0:04:35.\n",
      "  Batch 1,640  of  4,012.    Elapsed: 0:04:43.\n",
      "  Batch 1,680  of  4,012.    Elapsed: 0:04:52.\n",
      "  Batch 1,720  of  4,012.    Elapsed: 0:04:58.\n",
      "  Batch 1,760  of  4,012.    Elapsed: 0:05:05.\n",
      "  Batch 1,800  of  4,012.    Elapsed: 0:05:10.\n",
      "  Batch 1,840  of  4,012.    Elapsed: 0:05:18.\n",
      "  Batch 1,880  of  4,012.    Elapsed: 0:05:24.\n",
      "  Batch 1,920  of  4,012.    Elapsed: 0:05:31.\n",
      "  Batch 1,960  of  4,012.    Elapsed: 0:05:36.\n",
      "  Batch 2,000  of  4,012.    Elapsed: 0:05:43.\n",
      "  Batch 2,040  of  4,012.    Elapsed: 0:05:51.\n",
      "  Batch 2,080  of  4,012.    Elapsed: 0:05:58.\n",
      "  Batch 2,120  of  4,012.    Elapsed: 0:06:06.\n",
      "  Batch 2,160  of  4,012.    Elapsed: 0:06:13.\n",
      "  Batch 2,200  of  4,012.    Elapsed: 0:06:22.\n",
      "  Batch 2,240  of  4,012.    Elapsed: 0:06:30.\n",
      "  Batch 2,280  of  4,012.    Elapsed: 0:06:34.\n",
      "  Batch 2,320  of  4,012.    Elapsed: 0:06:43.\n",
      "  Batch 2,360  of  4,012.    Elapsed: 0:06:51.\n",
      "  Batch 2,400  of  4,012.    Elapsed: 0:06:58.\n",
      "  Batch 2,440  of  4,012.    Elapsed: 0:07:05.\n",
      "  Batch 2,480  of  4,012.    Elapsed: 0:07:10.\n",
      "  Batch 2,520  of  4,012.    Elapsed: 0:07:17.\n",
      "  Batch 2,560  of  4,012.    Elapsed: 0:07:24.\n",
      "  Batch 2,600  of  4,012.    Elapsed: 0:07:32.\n",
      "  Batch 2,640  of  4,012.    Elapsed: 0:07:39.\n",
      "  Batch 2,680  of  4,012.    Elapsed: 0:07:46.\n",
      "  Batch 2,720  of  4,012.    Elapsed: 0:07:55.\n",
      "  Batch 2,760  of  4,012.    Elapsed: 0:08:02.\n",
      "  Batch 2,800  of  4,012.    Elapsed: 0:08:10.\n",
      "  Batch 2,840  of  4,012.    Elapsed: 0:08:17.\n",
      "  Batch 2,880  of  4,012.    Elapsed: 0:08:23.\n",
      "  Batch 2,920  of  4,012.    Elapsed: 0:08:31.\n",
      "  Batch 2,960  of  4,012.    Elapsed: 0:08:39.\n",
      "  Batch 3,000  of  4,012.    Elapsed: 0:08:48.\n",
      "  Batch 3,040  of  4,012.    Elapsed: 0:08:54.\n",
      "  Batch 3,080  of  4,012.    Elapsed: 0:09:00.\n",
      "  Batch 3,120  of  4,012.    Elapsed: 0:09:08.\n",
      "  Batch 3,160  of  4,012.    Elapsed: 0:09:15.\n",
      "  Batch 3,200  of  4,012.    Elapsed: 0:09:22.\n",
      "  Batch 3,240  of  4,012.    Elapsed: 0:09:29.\n",
      "  Batch 3,280  of  4,012.    Elapsed: 0:09:36.\n",
      "  Batch 3,320  of  4,012.    Elapsed: 0:09:43.\n",
      "  Batch 3,360  of  4,012.    Elapsed: 0:09:49.\n",
      "  Batch 3,400  of  4,012.    Elapsed: 0:09:57.\n",
      "  Batch 3,440  of  4,012.    Elapsed: 0:10:03.\n",
      "  Batch 3,480  of  4,012.    Elapsed: 0:10:12.\n",
      "  Batch 3,520  of  4,012.    Elapsed: 0:10:18.\n",
      "  Batch 3,560  of  4,012.    Elapsed: 0:10:25.\n",
      "  Batch 3,600  of  4,012.    Elapsed: 0:10:31.\n",
      "  Batch 3,640  of  4,012.    Elapsed: 0:10:41.\n",
      "  Batch 3,680  of  4,012.    Elapsed: 0:10:48.\n",
      "  Batch 3,720  of  4,012.    Elapsed: 0:10:54.\n",
      "  Batch 3,760  of  4,012.    Elapsed: 0:11:01.\n",
      "  Batch 3,800  of  4,012.    Elapsed: 0:11:07.\n",
      "  Batch 3,840  of  4,012.    Elapsed: 0:11:13.\n",
      "  Batch 3,880  of  4,012.    Elapsed: 0:11:20.\n",
      "  Batch 3,920  of  4,012.    Elapsed: 0:11:29.\n",
      "  Batch 3,960  of  4,012.    Elapsed: 0:11:34.\n",
      "  Batch 4,000  of  4,012.    Elapsed: 0:11:39.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:11:41\n",
      "\n",
      "Running Validation...\n",
      "Saving model...\n",
      "  Validation Loss: 0.08\n",
      "  Validation took: 0:00:40\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  4,012.    Elapsed: 0:00:08.\n",
      "  Batch    80  of  4,012.    Elapsed: 0:00:16.\n",
      "  Batch   120  of  4,012.    Elapsed: 0:00:23.\n",
      "  Batch   160  of  4,012.    Elapsed: 0:00:29.\n",
      "  Batch   200  of  4,012.    Elapsed: 0:00:34.\n",
      "  Batch   240  of  4,012.    Elapsed: 0:00:42.\n",
      "  Batch   280  of  4,012.    Elapsed: 0:00:50.\n",
      "  Batch   320  of  4,012.    Elapsed: 0:00:58.\n",
      "  Batch   360  of  4,012.    Elapsed: 0:01:05.\n",
      "  Batch   400  of  4,012.    Elapsed: 0:01:12.\n",
      "  Batch   440  of  4,012.    Elapsed: 0:01:19.\n",
      "  Batch   480  of  4,012.    Elapsed: 0:01:25.\n",
      "  Batch   520  of  4,012.    Elapsed: 0:01:30.\n",
      "  Batch   560  of  4,012.    Elapsed: 0:01:36.\n",
      "  Batch   600  of  4,012.    Elapsed: 0:01:43.\n",
      "  Batch   640  of  4,012.    Elapsed: 0:01:49.\n",
      "  Batch   680  of  4,012.    Elapsed: 0:01:56.\n",
      "  Batch   720  of  4,012.    Elapsed: 0:02:03.\n",
      "  Batch   760  of  4,012.    Elapsed: 0:02:08.\n",
      "  Batch   800  of  4,012.    Elapsed: 0:02:14.\n",
      "  Batch   840  of  4,012.    Elapsed: 0:02:21.\n",
      "  Batch   880  of  4,012.    Elapsed: 0:02:28.\n",
      "  Batch   920  of  4,012.    Elapsed: 0:02:35.\n",
      "  Batch   960  of  4,012.    Elapsed: 0:02:42.\n",
      "  Batch 1,000  of  4,012.    Elapsed: 0:02:48.\n",
      "  Batch 1,040  of  4,012.    Elapsed: 0:02:55.\n",
      "  Batch 1,080  of  4,012.    Elapsed: 0:03:03.\n",
      "  Batch 1,120  of  4,012.    Elapsed: 0:03:10.\n",
      "  Batch 1,160  of  4,012.    Elapsed: 0:03:16.\n",
      "  Batch 1,200  of  4,012.    Elapsed: 0:03:22.\n",
      "  Batch 1,240  of  4,012.    Elapsed: 0:03:31.\n",
      "  Batch 1,280  of  4,012.    Elapsed: 0:03:39.\n",
      "  Batch 1,320  of  4,012.    Elapsed: 0:03:45.\n",
      "  Batch 1,360  of  4,012.    Elapsed: 0:03:52.\n",
      "  Batch 1,400  of  4,012.    Elapsed: 0:04:01.\n",
      "  Batch 1,440  of  4,012.    Elapsed: 0:04:06.\n",
      "  Batch 1,480  of  4,012.    Elapsed: 0:04:12.\n",
      "  Batch 1,520  of  4,012.    Elapsed: 0:04:18.\n",
      "  Batch 1,560  of  4,012.    Elapsed: 0:04:24.\n",
      "  Batch 1,600  of  4,012.    Elapsed: 0:04:30.\n",
      "  Batch 1,640  of  4,012.    Elapsed: 0:04:37.\n",
      "  Batch 1,680  of  4,012.    Elapsed: 0:04:45.\n",
      "  Batch 1,720  of  4,012.    Elapsed: 0:04:53.\n",
      "  Batch 1,760  of  4,012.    Elapsed: 0:05:00.\n",
      "  Batch 1,800  of  4,012.    Elapsed: 0:05:07.\n",
      "  Batch 1,840  of  4,012.    Elapsed: 0:05:15.\n",
      "  Batch 1,880  of  4,012.    Elapsed: 0:05:21.\n",
      "  Batch 1,920  of  4,012.    Elapsed: 0:05:28.\n",
      "  Batch 1,960  of  4,012.    Elapsed: 0:05:35.\n",
      "  Batch 2,000  of  4,012.    Elapsed: 0:05:42.\n",
      "  Batch 2,040  of  4,012.    Elapsed: 0:05:51.\n",
      "  Batch 2,080  of  4,012.    Elapsed: 0:06:00.\n",
      "  Batch 2,120  of  4,012.    Elapsed: 0:06:06.\n",
      "  Batch 2,160  of  4,012.    Elapsed: 0:06:15.\n",
      "  Batch 2,200  of  4,012.    Elapsed: 0:06:22.\n",
      "  Batch 2,240  of  4,012.    Elapsed: 0:06:28.\n",
      "  Batch 2,280  of  4,012.    Elapsed: 0:06:35.\n",
      "  Batch 2,320  of  4,012.    Elapsed: 0:06:42.\n",
      "  Batch 2,360  of  4,012.    Elapsed: 0:06:49.\n",
      "  Batch 2,400  of  4,012.    Elapsed: 0:06:57.\n",
      "  Batch 2,440  of  4,012.    Elapsed: 0:07:05.\n",
      "  Batch 2,480  of  4,012.    Elapsed: 0:07:10.\n",
      "  Batch 2,520  of  4,012.    Elapsed: 0:07:16.\n",
      "  Batch 2,560  of  4,012.    Elapsed: 0:07:23.\n",
      "  Batch 2,600  of  4,012.    Elapsed: 0:07:30.\n",
      "  Batch 2,640  of  4,012.    Elapsed: 0:07:38.\n",
      "  Batch 2,680  of  4,012.    Elapsed: 0:07:44.\n",
      "  Batch 2,720  of  4,012.    Elapsed: 0:07:52.\n",
      "  Batch 2,760  of  4,012.    Elapsed: 0:08:00.\n",
      "  Batch 2,800  of  4,012.    Elapsed: 0:08:06.\n",
      "  Batch 2,840  of  4,012.    Elapsed: 0:08:13.\n",
      "  Batch 2,880  of  4,012.    Elapsed: 0:08:20.\n",
      "  Batch 2,920  of  4,012.    Elapsed: 0:08:27.\n",
      "  Batch 2,960  of  4,012.    Elapsed: 0:08:34.\n",
      "  Batch 3,000  of  4,012.    Elapsed: 0:08:41.\n",
      "  Batch 3,040  of  4,012.    Elapsed: 0:08:48.\n",
      "  Batch 3,080  of  4,012.    Elapsed: 0:08:55.\n",
      "  Batch 3,120  of  4,012.    Elapsed: 0:09:03.\n",
      "  Batch 3,160  of  4,012.    Elapsed: 0:09:09.\n",
      "  Batch 3,200  of  4,012.    Elapsed: 0:09:15.\n",
      "  Batch 3,240  of  4,012.    Elapsed: 0:09:23.\n",
      "  Batch 3,280  of  4,012.    Elapsed: 0:09:29.\n",
      "  Batch 3,320  of  4,012.    Elapsed: 0:09:36.\n",
      "  Batch 3,360  of  4,012.    Elapsed: 0:09:41.\n",
      "  Batch 3,400  of  4,012.    Elapsed: 0:09:48.\n",
      "  Batch 3,440  of  4,012.    Elapsed: 0:09:53.\n",
      "  Batch 3,480  of  4,012.    Elapsed: 0:09:59.\n",
      "  Batch 3,520  of  4,012.    Elapsed: 0:10:05.\n",
      "  Batch 3,560  of  4,012.    Elapsed: 0:10:13.\n",
      "  Batch 3,600  of  4,012.    Elapsed: 0:10:21.\n",
      "  Batch 3,640  of  4,012.    Elapsed: 0:10:30.\n",
      "  Batch 3,680  of  4,012.    Elapsed: 0:10:34.\n",
      "  Batch 3,720  of  4,012.    Elapsed: 0:10:41.\n",
      "  Batch 3,760  of  4,012.    Elapsed: 0:10:49.\n",
      "  Batch 3,800  of  4,012.    Elapsed: 0:10:55.\n",
      "  Batch 3,840  of  4,012.    Elapsed: 0:11:03.\n",
      "  Batch 3,880  of  4,012.    Elapsed: 0:11:10.\n",
      "  Batch 3,920  of  4,012.    Elapsed: 0:11:17.\n",
      "  Batch 3,960  of  4,012.    Elapsed: 0:11:25.\n",
      "  Batch 4,000  of  4,012.    Elapsed: 0:11:31.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:11:33\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.08\n",
      "  Validation took: 0:00:40\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  4,012.    Elapsed: 0:00:07.\n",
      "  Batch    80  of  4,012.    Elapsed: 0:00:14.\n",
      "  Batch   120  of  4,012.    Elapsed: 0:00:21.\n",
      "  Batch   160  of  4,012.    Elapsed: 0:00:28.\n",
      "  Batch   200  of  4,012.    Elapsed: 0:00:35.\n",
      "  Batch   240  of  4,012.    Elapsed: 0:00:43.\n",
      "  Batch   280  of  4,012.    Elapsed: 0:00:48.\n",
      "  Batch   320  of  4,012.    Elapsed: 0:00:55.\n",
      "  Batch   360  of  4,012.    Elapsed: 0:01:03.\n",
      "  Batch   400  of  4,012.    Elapsed: 0:01:09.\n",
      "  Batch   440  of  4,012.    Elapsed: 0:01:16.\n",
      "  Batch   480  of  4,012.    Elapsed: 0:01:23.\n",
      "  Batch   520  of  4,012.    Elapsed: 0:01:30.\n",
      "  Batch   560  of  4,012.    Elapsed: 0:01:37.\n",
      "  Batch   600  of  4,012.    Elapsed: 0:01:44.\n",
      "  Batch   640  of  4,012.    Elapsed: 0:01:53.\n",
      "  Batch   680  of  4,012.    Elapsed: 0:02:00.\n",
      "  Batch   720  of  4,012.    Elapsed: 0:02:07.\n",
      "  Batch   760  of  4,012.    Elapsed: 0:02:14.\n",
      "  Batch   800  of  4,012.    Elapsed: 0:02:20.\n",
      "  Batch   840  of  4,012.    Elapsed: 0:02:27.\n",
      "  Batch   880  of  4,012.    Elapsed: 0:02:36.\n",
      "  Batch   920  of  4,012.    Elapsed: 0:02:43.\n",
      "  Batch   960  of  4,012.    Elapsed: 0:02:50.\n",
      "  Batch 1,000  of  4,012.    Elapsed: 0:02:57.\n",
      "  Batch 1,040  of  4,012.    Elapsed: 0:03:02.\n",
      "  Batch 1,080  of  4,012.    Elapsed: 0:03:10.\n",
      "  Batch 1,120  of  4,012.    Elapsed: 0:03:17.\n",
      "  Batch 1,160  of  4,012.    Elapsed: 0:03:25.\n",
      "  Batch 1,200  of  4,012.    Elapsed: 0:03:30.\n",
      "  Batch 1,240  of  4,012.    Elapsed: 0:03:36.\n",
      "  Batch 1,280  of  4,012.    Elapsed: 0:03:44.\n",
      "  Batch 1,320  of  4,012.    Elapsed: 0:03:49.\n",
      "  Batch 1,360  of  4,012.    Elapsed: 0:03:56.\n",
      "  Batch 1,400  of  4,012.    Elapsed: 0:04:03.\n",
      "  Batch 1,440  of  4,012.    Elapsed: 0:04:10.\n",
      "  Batch 1,480  of  4,012.    Elapsed: 0:04:17.\n",
      "  Batch 1,520  of  4,012.    Elapsed: 0:04:26.\n",
      "  Batch 1,560  of  4,012.    Elapsed: 0:04:33.\n",
      "  Batch 1,600  of  4,012.    Elapsed: 0:04:42.\n",
      "  Batch 1,640  of  4,012.    Elapsed: 0:04:51.\n",
      "  Batch 1,680  of  4,012.    Elapsed: 0:04:58.\n",
      "  Batch 1,720  of  4,012.    Elapsed: 0:05:06.\n",
      "  Batch 1,760  of  4,012.    Elapsed: 0:05:13.\n",
      "  Batch 1,800  of  4,012.    Elapsed: 0:05:19.\n",
      "  Batch 1,840  of  4,012.    Elapsed: 0:05:24.\n",
      "  Batch 1,880  of  4,012.    Elapsed: 0:05:31.\n",
      "  Batch 1,920  of  4,012.    Elapsed: 0:05:36.\n",
      "  Batch 1,960  of  4,012.    Elapsed: 0:05:44.\n",
      "  Batch 2,000  of  4,012.    Elapsed: 0:05:49.\n",
      "  Batch 2,040  of  4,012.    Elapsed: 0:05:55.\n",
      "  Batch 2,080  of  4,012.    Elapsed: 0:06:01.\n",
      "  Batch 2,120  of  4,012.    Elapsed: 0:06:07.\n",
      "  Batch 2,160  of  4,012.    Elapsed: 0:06:14.\n",
      "  Batch 2,200  of  4,012.    Elapsed: 0:06:20.\n",
      "  Batch 2,240  of  4,012.    Elapsed: 0:06:26.\n",
      "  Batch 2,280  of  4,012.    Elapsed: 0:06:35.\n",
      "  Batch 2,320  of  4,012.    Elapsed: 0:06:41.\n",
      "  Batch 2,360  of  4,012.    Elapsed: 0:06:48.\n",
      "  Batch 2,400  of  4,012.    Elapsed: 0:06:54.\n",
      "  Batch 2,440  of  4,012.    Elapsed: 0:07:01.\n",
      "  Batch 2,480  of  4,012.    Elapsed: 0:07:07.\n",
      "  Batch 2,520  of  4,012.    Elapsed: 0:07:14.\n",
      "  Batch 2,560  of  4,012.    Elapsed: 0:07:19.\n",
      "  Batch 2,600  of  4,012.    Elapsed: 0:07:25.\n",
      "  Batch 2,640  of  4,012.    Elapsed: 0:07:32.\n",
      "  Batch 2,680  of  4,012.    Elapsed: 0:07:40.\n",
      "  Batch 2,720  of  4,012.    Elapsed: 0:07:48.\n",
      "  Batch 2,760  of  4,012.    Elapsed: 0:07:55.\n",
      "  Batch 2,800  of  4,012.    Elapsed: 0:08:04.\n",
      "  Batch 2,840  of  4,012.    Elapsed: 0:08:12.\n",
      "  Batch 2,880  of  4,012.    Elapsed: 0:08:21.\n",
      "  Batch 2,920  of  4,012.    Elapsed: 0:08:28.\n",
      "  Batch 2,960  of  4,012.    Elapsed: 0:08:35.\n",
      "  Batch 3,000  of  4,012.    Elapsed: 0:08:40.\n",
      "  Batch 3,040  of  4,012.    Elapsed: 0:08:47.\n",
      "  Batch 3,080  of  4,012.    Elapsed: 0:08:53.\n",
      "  Batch 3,120  of  4,012.    Elapsed: 0:08:59.\n",
      "  Batch 3,160  of  4,012.    Elapsed: 0:09:05.\n",
      "  Batch 3,200  of  4,012.    Elapsed: 0:09:12.\n",
      "  Batch 3,240  of  4,012.    Elapsed: 0:09:18.\n",
      "  Batch 3,280  of  4,012.    Elapsed: 0:09:26.\n",
      "  Batch 3,320  of  4,012.    Elapsed: 0:09:32.\n",
      "  Batch 3,360  of  4,012.    Elapsed: 0:09:39.\n",
      "  Batch 3,400  of  4,012.    Elapsed: 0:09:45.\n",
      "  Batch 3,440  of  4,012.    Elapsed: 0:09:53.\n",
      "  Batch 3,480  of  4,012.    Elapsed: 0:09:59.\n",
      "  Batch 3,520  of  4,012.    Elapsed: 0:10:06.\n",
      "  Batch 3,560  of  4,012.    Elapsed: 0:10:13.\n",
      "  Batch 3,600  of  4,012.    Elapsed: 0:10:21.\n",
      "  Batch 3,640  of  4,012.    Elapsed: 0:10:26.\n",
      "  Batch 3,680  of  4,012.    Elapsed: 0:10:34.\n",
      "  Batch 3,720  of  4,012.    Elapsed: 0:10:41.\n",
      "  Batch 3,760  of  4,012.    Elapsed: 0:10:48.\n",
      "  Batch 3,800  of  4,012.    Elapsed: 0:10:55.\n",
      "  Batch 3,840  of  4,012.    Elapsed: 0:11:02.\n",
      "  Batch 3,880  of  4,012.    Elapsed: 0:11:11.\n",
      "  Batch 3,920  of  4,012.    Elapsed: 0:11:18.\n",
      "  Batch 3,960  of  4,012.    Elapsed: 0:11:25.\n",
      "  Batch 4,000  of  4,012.    Elapsed: 0:11:33.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:11:35\n",
      "\n",
      "Running Validation...\n",
      "Saving model...\n",
      "  Validation Loss: 0.08\n",
      "  Validation took: 0:00:39\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  4,012.    Elapsed: 0:00:08.\n",
      "  Batch    80  of  4,012.    Elapsed: 0:00:13.\n",
      "  Batch   120  of  4,012.    Elapsed: 0:00:20.\n",
      "  Batch   160  of  4,012.    Elapsed: 0:00:25.\n",
      "  Batch   200  of  4,012.    Elapsed: 0:00:33.\n",
      "  Batch   240  of  4,012.    Elapsed: 0:00:40.\n",
      "  Batch   280  of  4,012.    Elapsed: 0:00:46.\n",
      "  Batch   320  of  4,012.    Elapsed: 0:00:53.\n",
      "  Batch   360  of  4,012.    Elapsed: 0:01:01.\n",
      "  Batch   400  of  4,012.    Elapsed: 0:01:07.\n",
      "  Batch   440  of  4,012.    Elapsed: 0:01:13.\n",
      "  Batch   480  of  4,012.    Elapsed: 0:01:18.\n",
      "  Batch   520  of  4,012.    Elapsed: 0:01:25.\n",
      "  Batch   560  of  4,012.    Elapsed: 0:01:34.\n",
      "  Batch   600  of  4,012.    Elapsed: 0:01:40.\n",
      "  Batch   640  of  4,012.    Elapsed: 0:01:48.\n",
      "  Batch   680  of  4,012.    Elapsed: 0:01:53.\n",
      "  Batch   720  of  4,012.    Elapsed: 0:01:58.\n",
      "  Batch   760  of  4,012.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  4,012.    Elapsed: 0:02:13.\n",
      "  Batch   840  of  4,012.    Elapsed: 0:02:19.\n",
      "  Batch   880  of  4,012.    Elapsed: 0:02:26.\n",
      "  Batch   920  of  4,012.    Elapsed: 0:02:32.\n",
      "  Batch   960  of  4,012.    Elapsed: 0:02:39.\n",
      "  Batch 1,000  of  4,012.    Elapsed: 0:02:47.\n",
      "  Batch 1,040  of  4,012.    Elapsed: 0:02:52.\n",
      "  Batch 1,080  of  4,012.    Elapsed: 0:02:59.\n",
      "  Batch 1,120  of  4,012.    Elapsed: 0:03:07.\n",
      "  Batch 1,160  of  4,012.    Elapsed: 0:03:14.\n",
      "  Batch 1,200  of  4,012.    Elapsed: 0:03:22.\n",
      "  Batch 1,240  of  4,012.    Elapsed: 0:03:30.\n",
      "  Batch 1,280  of  4,012.    Elapsed: 0:03:37.\n",
      "  Batch 1,320  of  4,012.    Elapsed: 0:03:45.\n",
      "  Batch 1,360  of  4,012.    Elapsed: 0:03:50.\n",
      "  Batch 1,400  of  4,012.    Elapsed: 0:03:58.\n",
      "  Batch 1,440  of  4,012.    Elapsed: 0:04:04.\n",
      "  Batch 1,480  of  4,012.    Elapsed: 0:04:13.\n",
      "  Batch 1,520  of  4,012.    Elapsed: 0:04:22.\n",
      "  Batch 1,560  of  4,012.    Elapsed: 0:04:29.\n",
      "  Batch 1,600  of  4,012.    Elapsed: 0:04:37.\n",
      "  Batch 1,640  of  4,012.    Elapsed: 0:04:42.\n",
      "  Batch 1,680  of  4,012.    Elapsed: 0:04:49.\n",
      "  Batch 1,720  of  4,012.    Elapsed: 0:04:55.\n",
      "  Batch 1,760  of  4,012.    Elapsed: 0:05:02.\n",
      "  Batch 1,800  of  4,012.    Elapsed: 0:05:11.\n",
      "  Batch 1,840  of  4,012.    Elapsed: 0:05:16.\n",
      "  Batch 1,880  of  4,012.    Elapsed: 0:05:23.\n",
      "  Batch 1,920  of  4,012.    Elapsed: 0:05:30.\n",
      "  Batch 1,960  of  4,012.    Elapsed: 0:05:37.\n",
      "  Batch 2,000  of  4,012.    Elapsed: 0:05:43.\n",
      "  Batch 2,040  of  4,012.    Elapsed: 0:05:50.\n",
      "  Batch 2,080  of  4,012.    Elapsed: 0:05:58.\n",
      "  Batch 2,120  of  4,012.    Elapsed: 0:06:04.\n",
      "  Batch 2,160  of  4,012.    Elapsed: 0:06:10.\n",
      "  Batch 2,200  of  4,012.    Elapsed: 0:06:16.\n",
      "  Batch 2,240  of  4,012.    Elapsed: 0:06:22.\n",
      "  Batch 2,280  of  4,012.    Elapsed: 0:06:29.\n",
      "  Batch 2,320  of  4,012.    Elapsed: 0:06:36.\n",
      "  Batch 2,360  of  4,012.    Elapsed: 0:06:41.\n",
      "  Batch 2,400  of  4,012.    Elapsed: 0:06:48.\n",
      "  Batch 2,440  of  4,012.    Elapsed: 0:06:55.\n",
      "  Batch 2,480  of  4,012.    Elapsed: 0:07:03.\n",
      "  Batch 2,520  of  4,012.    Elapsed: 0:07:13.\n",
      "  Batch 2,560  of  4,012.    Elapsed: 0:07:23.\n",
      "  Batch 2,600  of  4,012.    Elapsed: 0:07:30.\n",
      "  Batch 2,640  of  4,012.    Elapsed: 0:07:35.\n",
      "  Batch 2,680  of  4,012.    Elapsed: 0:07:43.\n",
      "  Batch 2,720  of  4,012.    Elapsed: 0:07:49.\n",
      "  Batch 2,760  of  4,012.    Elapsed: 0:07:56.\n",
      "  Batch 2,800  of  4,012.    Elapsed: 0:08:04.\n",
      "  Batch 2,840  of  4,012.    Elapsed: 0:08:10.\n",
      "  Batch 2,880  of  4,012.    Elapsed: 0:08:18.\n",
      "  Batch 2,920  of  4,012.    Elapsed: 0:08:27.\n",
      "  Batch 2,960  of  4,012.    Elapsed: 0:08:33.\n",
      "  Batch 3,000  of  4,012.    Elapsed: 0:08:38.\n",
      "  Batch 3,040  of  4,012.    Elapsed: 0:08:46.\n",
      "  Batch 3,080  of  4,012.    Elapsed: 0:08:55.\n",
      "  Batch 3,120  of  4,012.    Elapsed: 0:09:03.\n",
      "  Batch 3,160  of  4,012.    Elapsed: 0:09:09.\n",
      "  Batch 3,200  of  4,012.    Elapsed: 0:09:16.\n",
      "  Batch 3,240  of  4,012.    Elapsed: 0:09:25.\n",
      "  Batch 3,280  of  4,012.    Elapsed: 0:09:31.\n",
      "  Batch 3,320  of  4,012.    Elapsed: 0:09:37.\n",
      "  Batch 3,360  of  4,012.    Elapsed: 0:09:44.\n",
      "  Batch 3,400  of  4,012.    Elapsed: 0:09:49.\n",
      "  Batch 3,440  of  4,012.    Elapsed: 0:09:56.\n",
      "  Batch 3,480  of  4,012.    Elapsed: 0:10:03.\n",
      "  Batch 3,520  of  4,012.    Elapsed: 0:10:11.\n",
      "  Batch 3,560  of  4,012.    Elapsed: 0:10:16.\n",
      "  Batch 3,600  of  4,012.    Elapsed: 0:10:25.\n",
      "  Batch 3,640  of  4,012.    Elapsed: 0:10:29.\n",
      "  Batch 3,680  of  4,012.    Elapsed: 0:10:37.\n",
      "  Batch 3,720  of  4,012.    Elapsed: 0:10:45.\n",
      "  Batch 3,760  of  4,012.    Elapsed: 0:10:51.\n",
      "  Batch 3,800  of  4,012.    Elapsed: 0:10:58.\n",
      "  Batch 3,840  of  4,012.    Elapsed: 0:11:04.\n",
      "  Batch 3,880  of  4,012.    Elapsed: 0:11:12.\n",
      "  Batch 3,920  of  4,012.    Elapsed: 0:11:19.\n",
      "  Batch 3,960  of  4,012.    Elapsed: 0:11:26.\n",
      "  Batch 4,000  of  4,012.    Elapsed: 0:11:31.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:11:34\n",
      "\n",
      "Running Validation...\n",
      "Saving model...\n",
      "  Validation Loss: 0.08\n",
      "  Validation took: 0:00:39\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:01:24 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0.1 * total_steps, num_training_steps = total_steps)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "early_stopping_patience = 3\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "  # ========================================\n",
    "  #               Training\n",
    "  # ========================================\n",
    "      \n",
    "  print(\"\")\n",
    "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "  print('Training...')\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "  total_train_loss = 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  # For each batch of training data...\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # Progress update every 40 batches.\n",
    "    if step % 40 == 0 and not step == 0:\n",
    "      elapsed = format_time(time.time() - t0)            \n",
    "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    b_labels = batch['labels'].to(device)\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Perform a forward pass (evaluate the model on this training batch).\n",
    "    output = model(b_input_ids, b_input_mask)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(output, b_labels)\n",
    "      \n",
    "    total_train_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "  # Calculate the average loss over all of the batches.\n",
    "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "  # Measure how long this epoch took.\n",
    "  training_time = format_time(time.time() - t0)\n",
    "\n",
    "  print(\"\")\n",
    "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "  print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "  \n",
    "  # ========================================\n",
    "  #               Validation\n",
    "  # ========================================\n",
    "  # After the completion of each training epoch, measure our performance on\n",
    "  # our validation set.\n",
    "\n",
    "  print(\"\")\n",
    "  print(\"Running Validation...\")\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  total_eval_accuracy = 0\n",
    "  total_eval_loss = 0\n",
    "  nb_eval_steps = 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in val_dataloader:\n",
    "\n",
    "     \n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    b_labels = batch['labels'].to(device)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # Perform a forward pass\n",
    "      output = model(b_input_ids, b_input_mask)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(output.view(-1), b_labels.view(-1))\n",
    "\n",
    "    # Accumulate the development loss.\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over all of the batches.\n",
    "  avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "\n",
    "  # Measure how long the development run took.\n",
    "  validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "  if avg_val_loss < best_val_loss:\n",
    "    best_val_loss = avg_val_loss\n",
    "    epochs_without_improvement = 0\n",
    "    print(\"Saving model...\")\n",
    "    torch.save(model.state_dict(), \"C:/Users/youss/Desktop/MATERIAL/semester VIII/models/IMPORTANT REGRESSORS/roberta pandora/roberta-pandora-model.pth\")\n",
    "  else:\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "      print(f'Early stopping triggered after {epoch_i} epochs without improvement.')\n",
    "      break\n",
    "\n",
    "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "  print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "  # Record all statistics from this epoch.\n",
    "  training_stats.append(\n",
    "      {\n",
    "          'epoch': epoch_i + 1,\n",
    "          'Training Loss': avg_train_loss,\n",
    "          'Training Time': training_time,\n",
    "          'Valid. Loss': avg_val_loss,\n",
    "          'Validation Time': validation_time\n",
    "      })\n",
    "  \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.083938</td>\n",
       "      <td>0:11:42</td>\n",
       "      <td>0.080704</td>\n",
       "      <td>0:00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078803</td>\n",
       "      <td>0:11:41</td>\n",
       "      <td>0.079507</td>\n",
       "      <td>0:00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078381</td>\n",
       "      <td>0:11:33</td>\n",
       "      <td>0.079507</td>\n",
       "      <td>0:00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.077948</td>\n",
       "      <td>0:11:35</td>\n",
       "      <td>0.078914</td>\n",
       "      <td>0:00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.077493</td>\n",
       "      <td>0:11:34</td>\n",
       "      <td>0.078884</td>\n",
       "      <td>0:00:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss Training Time  Valid. Loss Validation Time\n",
       "epoch                                                          \n",
       "1           0.083938       0:11:42     0.080704         0:00:40\n",
       "2           0.078803       0:11:41     0.079507         0:00:40\n",
       "3           0.078381       0:11:33     0.079507         0:00:40\n",
       "4           0.077948       0:11:35     0.078914         0:00:39\n",
       "5           0.077493       0:11:34     0.078884         0:00:39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with three decimal places.\n",
    "#pd.set_option('precision', 4)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting values for 2,415 test sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_13976\\86404554.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07737786523899101\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting values for {:,} test sentences...'.format(len(test_dataloader.dataset)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "total_test_loss = 0\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "\n",
    "  # Add batch to GPU\n",
    "  b_input_ids = batch['input_ids'].to(device)\n",
    "  b_input_mask = batch['attention_mask'].to(device)\n",
    "  b_labels = batch['labels'].to(device)\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    \n",
    "    # Perform a forward pass\n",
    "    output = model(b_input_ids, b_input_mask)\n",
    "\n",
    "  # loss\n",
    "  loss = criterion(output.view(-1), b_labels.view(-1))\n",
    "\n",
    "  # Accumulate the development loss.\n",
    "  total_test_loss += loss.item()\n",
    "\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "\n",
    "print(avg_test_loss)\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    encoded_input = tokenizer(input, padding=True, truncation=True, return_tensors='pt')\n",
    "    model.to('cpu')\n",
    "    output = model(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3939, 0.6891, 0.3256, 0.3559, 0.4725]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I can understand about having millions of dollars. There's meaningful freedom that comes with that, but once you get much beyond that I have to tell you, it's the same hamburger.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"C:/Users/youss/Desktop/MATERIAL/semester VIII/models/roberta_clean_pandora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
